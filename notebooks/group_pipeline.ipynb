{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1a844d",
   "metadata": {},
   "source": [
    "# URL PhishGuard: Complete Feature Engineering Pipeline\n",
    "\n",
    "## Integrated Pipeline Overview\n",
    "\n",
    "This notebook executes the complete phishing detection feature engineering pipeline, integrating all team member contributions:\n",
    "\n",
    "**Pipeline Flow:**\n",
    "1. **M1 (IT24103625)**: URL Length & Hostname Length → `m1_url_length_features.csv`\n",
    "2. **M2 (IT24100950)**: Character Counts (uses M1 output) → `m2_character_features.csv`\n",
    "3. **M3 (IT24103925)**: IP Detection & Digit Density (uses M2 output) → `m3_ip_features.csv`\n",
    "4. **M4 (IT24103016)**: Subdomain & Path Depth (uses M3 output) → `m4_structure_features.csv`\n",
    "5. **M5 (IT24100659)**: MinMax Scaling (uses M4 output) → `m5_scaled_features.csv`\n",
    "6. **M6 (IT24104208)**: Feature Selection & Label Encoding (uses M5 output) → `m6_final_*.csv`\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Sequential Processing**: Each member builds on previous work\n",
    "- **No Duplication**: Each module focuses on specific feature types\n",
    "- **Maintainable**: Clear separation of concerns\n",
    "- **Scalable**: Easy to add new feature engineering steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for pipeline execution\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Set up environment\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 URL PhishGuard Pipeline - Starting Complete Analysis\")\n",
    "print(f\"📅 Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaac42",
   "metadata": {},
   "source": [
    "## Pipeline Configuration & Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b264fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline configuration\n",
    "pipeline_config = {\n",
    "    'data_path': '../data/raw/phishing_site_urls.csv',\n",
    "    'output_dir': '../results/outputs',\n",
    "    'modules': [\n",
    "        {'id': 'M1', 'name': 'URL Length Analysis', 'student': 'IT24103625', 'output': 'm1_url_length_features.csv'},\n",
    "        {'id': 'M2', 'name': 'Character Counts', 'student': 'IT24100950', 'output': 'm2_character_features.csv'},\n",
    "        {'id': 'M3', 'name': 'IP Detection', 'student': 'IT24103925', 'output': 'm3_ip_features.csv'},\n",
    "        {'id': 'M4', 'name': 'Structure Analysis', 'student': 'IT24103016', 'output': 'm4_structure_features.csv'},\n",
    "        {'id': 'M5', 'name': 'Scaling & Normalization', 'student': 'IT24100659', 'output': 'm5_scaled_features.csv'},\n",
    "        {'id': 'M6', 'name': 'Feature Selection', 'student': 'IT24104208', 'output': 'm6_final_selected_features.csv'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(pipeline_config['output_dir'], exist_ok=True)\n",
    "\n",
    "# Validate initial dataset\n",
    "print(\"📊 Initial Dataset Validation\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if os.path.exists(pipeline_config['data_path']):\n",
    "    df_initial = pd.read_csv(pipeline_config['data_path'])\n",
    "    print(f\"✅ Dataset loaded successfully\")\n",
    "    print(f\"   📈 Shape: {df_initial.shape}\")\n",
    "    print(f\"   📋 Columns: {df_initial.columns.tolist()}\")\n",
    "    print(f\"   🎯 Labels: {df_initial['Label'].value_counts().to_dict()}\")\n",
    "    print(f\"   🔍 Missing values: {df_initial.isnull().sum().sum()}\")\n",
    "    print(f\"   💾 Data size: {df_initial.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"❌ Dataset not found at: {pipeline_config['data_path']}\")\n",
    "\n",
    "print(f\"\\n🔧 Pipeline Configuration:\")\n",
    "for module in pipeline_config['modules']:\n",
    "    print(f\"   {module['id']}: {module['name']} ({module['student']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682a433",
   "metadata": {},
   "source": [
    "## Pipeline Execution Status Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba536145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pipeline execution status\n",
    "def check_pipeline_status():\n",
    "    print(\"🔍 Checking Pipeline Execution Status\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    pipeline_status = []\n",
    "    \n",
    "    for module in pipeline_config['modules']:\n",
    "        output_path = os.path.join(pipeline_config['output_dir'], module['output'])\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            df_temp = pd.read_csv(output_path)\n",
    "            status = \"✅ Complete\"\n",
    "            details = f\"Shape: {df_temp.shape}\"\n",
    "        else:\n",
    "            status = \"❌ Missing\"\n",
    "            details = \"Output file not found\"\n",
    "        \n",
    "        pipeline_status.append({\n",
    "            'Module': module['id'],\n",
    "            'Name': module['name'],\n",
    "            'Student': module['student'],\n",
    "            'Status': status,\n",
    "            'Details': details\n",
    "        })\n",
    "        \n",
    "        print(f\"{module['id']}: {module['name']:<25} {status} - {details}\")\n",
    "    \n",
    "    return pipeline_status\n",
    "\n",
    "status_report = check_pipeline_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec6be8c",
   "metadata": {},
   "source": [
    "## Execute Individual Modules\n",
    "\n",
    "**Note**: Run individual notebooks first if outputs are missing:\n",
    "1. `IT24103625_M1_URL_Length.ipynb`\n",
    "2. `IT24100950_M2_Char_Counts.ipynb`\n",
    "3. `IT24103925_M3_IP_Detection.ipynb`\n",
    "4. `IT24103016_M4_Depth_Count.ipynb`\n",
    "5. `IT24100659_M5_Scaling_Norm.ipynb`\n",
    "6. `IT24104208_M6_Selection_Encoding.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze each pipeline stage\n",
    "def analyze_pipeline_progression():\n",
    "    print(\"📊 Pipeline Progression Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    pipeline_data = {}\n",
    "    \n",
    "    # Load each stage's output if available\n",
    "    for module in pipeline_config['modules']:\n",
    "        output_path = os.path.join(pipeline_config['output_dir'], module['output'])\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            df = pd.read_csv(output_path)\n",
    "            \n",
    "            # Analyze feature growth\n",
    "            numeric_cols = [col for col in df.columns if col not in ['URL', 'Label', 'Label_Encoded']]\n",
    "            \n",
    "            pipeline_data[module['id']] = {\n",
    "                'data': df,\n",
    "                'shape': df.shape,\n",
    "                'features': numeric_cols,\n",
    "                'feature_count': len(numeric_cols)\n",
    "            }\n",
    "            \n",
    "            print(f\"{module['id']}: {module['name']}\")\n",
    "            print(f\"   📏 Shape: {df.shape}\")\n",
    "            print(f\"   🔢 Features: {len(numeric_cols)}\")\n",
    "            \n",
    "            if len(numeric_cols) <= 10:\n",
    "                print(f\"   📋 Feature List: {numeric_cols}\")\n",
    "            else:\n",
    "                print(f\"   📋 Sample Features: {numeric_cols[:5]} ... (+{len(numeric_cols)-5} more)\")\n",
    "            \n",
    "            print()\n",
    "        else:\n",
    "            print(f\"⚠️ {module['id']} output not found - run {module['id']} notebook first\")\n",
    "    \n",
    "    return pipeline_data\n",
    "\n",
    "pipeline_data = analyze_pipeline_progression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e16e4",
   "metadata": {},
   "source": [
    "## Pipeline Visualization & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pipeline progression\n",
    "if len(pipeline_data) > 0:\n",
    "    print(\"📈 Creating Pipeline Visualization\")\n",
    "    \n",
    "    # Create comprehensive pipeline visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Feature Count Progression\n",
    "    modules_completed = list(pipeline_data.keys())\n",
    "    feature_counts = [pipeline_data[module]['feature_count'] for module in modules_completed]\n",
    "    \n",
    "    ax1.plot(modules_completed, feature_counts, 'bo-', linewidth=3, markersize=8)\n",
    "    ax1.fill_between(modules_completed, feature_counts, alpha=0.3)\n",
    "    ax1.set_xlabel('Pipeline Module', fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Features', fontweight='bold')\n",
    "    ax1.set_title('Feature Engineering Progression', fontweight='bold', pad=20)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (module, count) in enumerate(zip(modules_completed, feature_counts)):\n",
    "        ax1.annotate(f'{count}', (i, count), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Data Quality Check (if final data available)\n",
    "    if 'M6' in pipeline_data:\n",
    "        final_data = pipeline_data['M6']['data']\n",
    "        label_dist = final_data['Label'].value_counts()\n",
    "        \n",
    "        colors = ['#2E8B57', '#DC143C']\n",
    "        wedges, texts, autotexts = ax2.pie(label_dist.values, labels=label_dist.index, \n",
    "                                          autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "        ax2.set_title('Final Dataset Label Distribution', fontweight='bold')\n",
    "        \n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "    \n",
    "    # 3. Feature Types Analysis\n",
    "    if 'M6' in pipeline_data:\n",
    "        feature_categories = {\n",
    "            'Length Features': ['url_length', 'hostname_length'],\n",
    "            'Character Counts': ['count_at', 'count_double_slash', 'count_dash', 'count_underscore'],\n",
    "            'IP & Digits': ['has_ip', 'digit_density'],\n",
    "            'Structure': ['subdomain_count', 'path_depth', 'query_params_count'],\n",
    "            'Other': []\n",
    "        }\n",
    "        \n",
    "        all_features = pipeline_data['M6']['features']\n",
    "        categorized = set()\n",
    "        \n",
    "        for category, features in feature_categories.items():\n",
    "            count = sum(1 for f in features if f in all_features)\n",
    "            categorized.update(f for f in features if f in all_features)\n",
    "            feature_categories[category] = count\n",
    "        \n",
    "        # Count uncategorized features\n",
    "        feature_categories['Other'] = len(all_features) - len(categorized)\n",
    "        \n",
    "        categories = list(feature_categories.keys())\n",
    "        counts = list(feature_categories.values())\n",
    "        \n",
    "        bars = ax3.bar(categories, counts, color=plt.cm.Set3(np.linspace(0, 1, len(categories))))\n",
    "        ax3.set_xlabel('Feature Category', fontweight='bold')\n",
    "        ax3.set_ylabel('Feature Count', fontweight='bold')\n",
    "        ax3.set_title('Feature Categories Distribution', fontweight='bold')\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Pipeline Execution Status\n",
    "    module_names = [m['id'] for m in pipeline_config['modules']]\n",
    "    completion_status = [1 if m['id'] in pipeline_data else 0 for m in pipeline_config['modules']]\n",
    "    \n",
    "    colors_status = ['green' if status else 'red' for status in completion_status]\n",
    "    bars = ax4.bar(module_names, completion_status, color=colors_status, alpha=0.7)\n",
    "    ax4.set_xlabel('Pipeline Modules', fontweight='bold')\n",
    "    ax4.set_ylabel('Completion Status', fontweight='bold')\n",
    "    ax4.set_title('Module Execution Status', fontweight='bold')\n",
    "    ax4.set_ylim(0, 1.2)\n",
    "    \n",
    "    # Add status labels\n",
    "    for i, (bar, status) in enumerate(zip(bars, completion_status)):\n",
    "        label = \"✅\" if status else \"❌\"\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., 0.5, label,\n",
    "                ha='center', va='center', fontsize=20)\n",
    "    \n",
    "    plt.suptitle('URL PhishGuard: Complete Pipeline Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No pipeline outputs found. Please run individual module notebooks first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024313f",
   "metadata": {},
   "source": [
    "## Final Pipeline Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive pipeline summary\n",
    "def generate_pipeline_summary():\n",
    "    print(\"📋 FINAL PIPELINE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"📅 Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"📊 Initial Dataset: {df_initial.shape[0]:,} URLs\")\n",
    "    \n",
    "    completed_modules = len(pipeline_data)\n",
    "    total_modules = len(pipeline_config['modules'])\n",
    "    completion_rate = (completed_modules / total_modules) * 100\n",
    "    \n",
    "    print(f\"\\n🚀 PIPELINE EXECUTION:\")\n",
    "    print(f\"   ✅ Completed Modules: {completed_modules}/{total_modules} ({completion_rate:.0f}%)\")\n",
    "    \n",
    "    if completed_modules == total_modules:\n",
    "        final_data = pipeline_data['M6']['data']\n",
    "        final_features = pipeline_data['M6']['features']\n",
    "        \n",
    "        print(f\"   🎯 Final Dataset: {final_data.shape}\")\n",
    "        print(f\"   🔢 Total Features: {len(final_features)}\")\n",
    "        print(f\"   📊 Label Distribution: {final_data['Label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Feature engineering summary\n",
    "        print(f\"\\n🔧 FEATURE ENGINEERING BREAKDOWN:\")\n",
    "        for module in pipeline_config['modules']:\n",
    "            if module['id'] in pipeline_data:\n",
    "                feature_count = pipeline_data[module['id']]['feature_count']\n",
    "                print(f\"   {module['id']}: {feature_count:2d} features - {module['name']}\")\n",
    "        \n",
    "        print(f\"\\n📈 PIPELINE EFFECTIVENESS:\")\n",
    "        initial_features = 0  # Started with just URL and Label\n",
    "        final_feature_count = len(final_features)\n",
    "        print(f\"   📊 Features Created: {final_feature_count} features\")\n",
    "        print(f\"   🎯 Data Transformation: Raw URLs → ML-ready features\")\n",
    "        print(f\"   ⚖️ Feature Scaling: Applied (MinMax normalization)\")\n",
    "        print(f\"   🔍 Feature Selection: Applied (Chi-squared test)\")\n",
    "        print(f\"   🏷️ Label Encoding: Applied (good=0, bad=1)\")\n",
    "        \n",
    "        print(f\"\\n✅ PIPELINE STATUS: COMPLETE - Ready for Machine Learning\")\n",
    "        print(f\"\\n📁 OUTPUT FILES:\")\n",
    "        for module in pipeline_config['modules']:\n",
    "            if module['id'] in pipeline_data:\n",
    "                print(f\"   📄 {module['output']}\")\n",
    "        \n",
    "        print(f\"\\n🚀 NEXT STEPS:\")\n",
    "        print(f\"   1. Load final dataset: 'm6_final_selected_features.csv'\")\n",
    "        print(f\"   2. Split into training/testing sets\")\n",
    "        print(f\"   3. Train machine learning models (Random Forest, SVM, etc.)\")\n",
    "        print(f\"   4. Evaluate model performance\")\n",
    "        print(f\"   5. Deploy phishing detection system\")\n",
    "        \n",
    "    else:\n",
    "        missing_modules = [m['id'] for m in pipeline_config['modules'] if m['id'] not in pipeline_data]\n",
    "        print(f\"   ❌ Missing Modules: {missing_modules}\")\n",
    "        print(f\"\\n⚠️ ACTION REQUIRED:\")\n",
    "        print(f\"   Please run the following notebooks to complete the pipeline:\")\n",
    "        for module in pipeline_config['modules']:\n",
    "            if module['id'] not in pipeline_data:\n",
    "                notebook_name = f\"{module['student']}_{module['id']}_*.ipynb\"\n",
    "                print(f\"   📔 {notebook_name}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "\n",
    "generate_pipeline_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489708db",
   "metadata": {},
   "source": [
    "## Quick ML Model Demo (if pipeline complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick ML demonstration if pipeline is complete\n",
    "if 'M6' in pipeline_data:\n",
    "    print(\"🤖 QUICK MACHINE LEARNING DEMO\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        \n",
    "        # Load final dataset\n",
    "        final_data = pipeline_data['M6']['data']\n",
    "        \n",
    "        # Prepare features and target\n",
    "        feature_columns = [col for col in final_data.columns if col not in ['URL', 'Label', 'Label_Encoded']]\n",
    "        X = final_data[feature_columns]\n",
    "        \n",
    "        # Use label-encoded target or create it\n",
    "        if 'Label_Encoded' in final_data.columns:\n",
    "            y = final_data['Label_Encoded']\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(final_data['Label'])\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Training Data: {X_train.shape}\")\n",
    "        print(f\"📊 Testing Data: {X_test.shape}\")\n",
    "        print(f\"🔢 Features Used: {len(feature_columns)}\")\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        print(f\"\\n🌲 Training Random Forest Classifier...\")\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = (y_pred == y_test).mean()\n",
    "        print(f\"✅ Model Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Importance': rf_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n🔍 Top 5 Most Important Features:\")\n",
    "        for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):\n",
    "            print(f\"   {i}. {row['Feature']:<20} ({row['Importance']:.4f})\")\n",
    "        \n",
    "        print(f\"\\n🎯 PIPELINE SUCCESS: Features successfully predict phishing URLs!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠️ scikit-learn not available for ML demo\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ ML demo error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"🤖 ML Demo: Complete pipeline required (run all M1-M6 notebooks first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🏁 PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"🎯 URL PhishGuard feature engineering pipeline analysis finished\")\n",
    "print(f\"📅 Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
