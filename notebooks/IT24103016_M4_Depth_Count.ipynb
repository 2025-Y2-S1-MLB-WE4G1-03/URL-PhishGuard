{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a333fc9b",
   "metadata": {},
   "source": [
    "# M4 - Subdomain & Path Depth Analysis\n",
    "\n",
    "**Student ID**: IT24103016  \n",
    "**Focus**: Count subdomains and directory depth in URL paths  \n",
    "**Visualization**: Violin plot comparing subdomain count distributions  \n",
    "**Input**: M3 output (IP Detection features)  \n",
    "**Output**: M4 features for M5 (Scaling)\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Extract subdomain count and path depth features\n",
    "2. Analyze URL structure complexity patterns\n",
    "3. Create violin plot visualization for subdomain distribution\n",
    "4. Save processed data for M5 (Normalization & Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from M3 (IP Detection Analysis)\n",
    "m3_output_path = '../results/outputs/m3_ip_features.csv'\n",
    "\n",
    "if os.path.exists(m3_output_path):\n",
    "    print(\"ğŸ“‚ Loading M3 output (IP Detection features)...\")\n",
    "    df = pd.read_csv(m3_output_path)\n",
    "    print(f\"âœ… Loaded M3 data: {df.shape}\")\n",
    "    print(f\"Existing features: {df.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"âš ï¸ M3 output not found, loading raw data...\")\n",
    "    df = pd.read_csv('../data/raw/phishing_site_urls.csv')\n",
    "    print(f\"Loaded raw data: {df.shape}\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Label distribution:\\n{df['Label'].value_counts()}\")\n",
    "print(f\"\\nSample URLs:\")\n",
    "print(df[['URL', 'Label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d0347f",
   "metadata": {},
   "source": [
    "## Feature Engineering: Subdomain and Path Depth Analysis\n",
    "\n",
    "Extract subdomain count and path depth features that capture URL structure complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structure_features(url):\n",
    "    \"\"\"\n",
    "    Extract subdomain count and path depth from URL\n",
    "    Returns: tuple (subdomain_count, path_depth, query_params_count)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse URL components\n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        # Extract hostname (netloc)\n",
    "        hostname = parsed.netloc if parsed.netloc else url.split('/')[0]\n",
    "        \n",
    "        # Remove port number if present\n",
    "        if ':' in hostname:\n",
    "            hostname = hostname.split(':')[0]\n",
    "        \n",
    "        # Count subdomains\n",
    "        # Standard domains have format: subdomain.domain.tld\n",
    "        # Count dots and subtract 1 for the main domain separation\n",
    "        if '.' in hostname and hostname.count('.') >= 1:\n",
    "            # Handle special cases like IP addresses\n",
    "            if hostname.replace('.', '').isdigit():\n",
    "                subdomain_count = 0  # IP addresses don't have subdomains\n",
    "            else:\n",
    "                subdomain_count = max(0, hostname.count('.') - 1)\n",
    "        else:\n",
    "            subdomain_count = 0\n",
    "            \n",
    "        # Count path depth (directory levels)\n",
    "        path = parsed.path if parsed.path else '/'\n",
    "        # Remove leading and trailing slashes, split by slash\n",
    "        path_parts = [part for part in path.strip('/').split('/') if part]\n",
    "        path_depth = len(path_parts)\n",
    "        \n",
    "        # Count query parameters\n",
    "        query = parsed.query if parsed.query else ''\n",
    "        query_params_count = query.count('&') + (1 if query else 0)\n",
    "        if query == '':\n",
    "            query_params_count = 0\n",
    "            \n",
    "        # Additional structural features\n",
    "        # Fragment count (after #)\n",
    "        has_fragment = 1 if parsed.fragment else 0\n",
    "        \n",
    "        # URL segments (total parts separated by /)\n",
    "        url_segments = len([part for part in url.split('/') if part])\n",
    "        \n",
    "        return subdomain_count, path_depth, query_params_count, has_fragment, url_segments\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL: {url[:50]}... - {e}\")\n",
    "        return 0, 0, 0, 0, 0\n",
    "\n",
    "# Test the function with sample URLs\n",
    "test_urls = [\n",
    "    'https://www.google.com',\n",
    "    'https://mail.google.com/mail/u/0/inbox',\n",
    "    'http://suspicious.sub.domain.phishing-site.com/login/verify/account/update.php?id=123&token=abc',\n",
    "    'https://github.com/user/repository/issues/123',\n",
    "    'http://192.168.1.1/admin',\n",
    "    'https://very.long.subdomain.chain.example.com/deep/path/structure/file.html#section1'\n",
    "]\n",
    "\n",
    "print(\"Testing structure extraction function:\")\n",
    "print(f\"{'URL':<60} {'Subdomains':<10} {'Path Depth':<10} {'Query Params':<12} {'Fragment':<8} {'Segments':<8}\")\n",
    "print(\"-\" * 110)\n",
    "for url in test_urls:\n",
    "    subdomain_cnt, path_d, query_cnt, fragment, segments = extract_structure_features(url)\n",
    "    print(f\"{url[:58]:<60} {subdomain_cnt:<10} {path_d:<10} {query_cnt:<12} {fragment:<8} {segments:<8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0a361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply structure feature extraction to all URLs\n",
    "print(\"ğŸ”§ M4 Feature Engineering: Extracting URL structure features...\")\n",
    "\n",
    "# Extract features for all URLs\n",
    "structure_features = df['URL'].apply(extract_structure_features)\n",
    "df['subdomain_count'] = [feat[0] for feat in structure_features]\n",
    "df['path_depth'] = [feat[1] for feat in structure_features]\n",
    "df['query_params_count'] = [feat[2] for feat in structure_features]\n",
    "df['has_fragment'] = [feat[3] for feat in structure_features]\n",
    "df['url_segments'] = [feat[4] for feat in structure_features]\n",
    "\n",
    "print(\"âœ… M4 Features extracted:\")\n",
    "m4_features = ['subdomain_count', 'path_depth', 'query_params_count', 'has_fragment', 'url_segments']\n",
    "print(f\"   {m4_features}\")\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\nSample of extracted M4 features:\")\n",
    "sample_cols = ['URL', 'Label'] + m4_features\n",
    "sample_df = df[sample_cols].head(10)\n",
    "print(sample_df.to_string(index=False))\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nM4 Feature Statistics:\")\n",
    "print(df[m4_features].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473bc8d",
   "metadata": {},
   "source": [
    "## Data Analysis & Statistical Comparison\n",
    "\n",
    "Analyze the relationship between URL structure features and phishing labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis by label\n",
    "print(\"ğŸ“Š M4 Analysis: URL Structure Features by Label\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "structure_analysis = df.groupby('Label')[m4_features].agg(['mean', 'std', 'median', 'max']).round(3)\n",
    "print(structure_analysis)\n",
    "\n",
    "# Calculate differences between good and bad URLs\n",
    "good_urls = df[df['Label'] == 'good']\n",
    "bad_urls = df[df['Label'] == 'bad']\n",
    "\n",
    "print(f\"\\nğŸ” Detailed Comparison (Bad - Good URLs):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for feature in m4_features:\n",
    "    good_mean = good_urls[feature].mean()\n",
    "    bad_mean = bad_urls[feature].mean()\n",
    "    difference = bad_mean - good_mean\n",
    "    \n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Good URLs: {good_mean:.3f} | Bad URLs: {bad_mean:.3f} | Diff: {difference:+.3f}\")\n",
    "\n",
    "# Correlation analysis\n",
    "print(f\"\\nğŸ“ˆ Feature Correlations:\")\n",
    "correlation_matrix = df[m4_features].corr()\n",
    "print(correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186935cc",
   "metadata": {},
   "source": [
    "## Visualization: Required Violin Plot\n",
    "\n",
    "Create the main visualization: Violin plot comparing subdomain count distributions between Good/Bad URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization with required violin plot\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Main required visualization: Violin plot for subdomain count (takes 2 columns)\n",
    "ax1 = plt.subplot(2, 3, (1, 2))\n",
    "violin_data = [df[df['Label'] == 'good']['subdomain_count'], \n",
    "               df[df['Label'] == 'bad']['subdomain_count']]\n",
    "\n",
    "# Create violin plot\n",
    "parts = ax1.violinplot(violin_data, positions=[1, 2], showmeans=True, showmedians=True)\n",
    "\n",
    "# Customize violin plot colors\n",
    "colors = ['#2E8B57', '#DC143C']  # Green for good, red for bad\n",
    "for i, pc in enumerate(parts['bodies']):\n",
    "    pc.set_facecolor(colors[i])\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "ax1.set_xticks([1, 2])\n",
    "ax1.set_xticklabels(['Good URLs', 'Bad URLs'])\n",
    "ax1.set_ylabel('Subdomain Count', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Subdomain Count Distribution by Label\\n(M4 Required Visualization)', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add statistical annotations\n",
    "good_sub_mean = good_urls['subdomain_count'].mean()\n",
    "bad_sub_mean = bad_urls['subdomain_count'].mean()\n",
    "good_sub_median = good_urls['subdomain_count'].median()\n",
    "bad_sub_median = bad_urls['subdomain_count'].median()\n",
    "\n",
    "ax1.text(0.02, 0.98, \n",
    "         f'Good URLs:\\n  Mean: {good_sub_mean:.2f}\\n  Median: {good_sub_median}\\n\\n'\n",
    "         f'Bad URLs:\\n  Mean: {bad_sub_mean:.2f}\\n  Median: {bad_sub_median}\\n\\n'\n",
    "         f'Difference: {bad_sub_mean-good_sub_mean:+.2f}', \n",
    "         transform=ax1.transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Path Depth Box Plot\n",
    "ax2 = plt.subplot(2, 3, 3)\n",
    "path_data = [df[df['Label'] == 'good']['path_depth'], \n",
    "             df[df['Label'] == 'bad']['path_depth']]\n",
    "box_plot = ax2.boxplot(path_data, labels=['Good', 'Bad'], patch_artist=True)\n",
    "for i, box in enumerate(box_plot['boxes']):\n",
    "    box.set_facecolor(colors[i])\n",
    "    box.set_alpha(0.7)\n",
    "\n",
    "ax2.set_ylabel('Path Depth', fontweight='bold')\n",
    "ax2.set_title('Path Depth Distribution', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Query Parameters Distribution\n",
    "ax3 = plt.subplot(2, 3, 4)\n",
    "query_data = [df[df['Label'] == 'good']['query_params_count'], \n",
    "              df[df['Label'] == 'bad']['query_params_count']]\n",
    "ax3.hist([query_data[0], query_data[1]], bins=range(0, min(15, max(df['query_params_count'])+2)), \n",
    "         alpha=0.7, label=['Good URLs', 'Bad URLs'], color=colors)\n",
    "ax3.set_xlabel('Query Parameters Count', fontweight='bold')\n",
    "ax3.set_ylabel('Frequency', fontweight='bold')\n",
    "ax3.set_title('Query Parameters Distribution', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# URL Segments Distribution  \n",
    "ax4 = plt.subplot(2, 3, 5)\n",
    "segments_data = [df[df['Label'] == 'good']['url_segments'], \n",
    "                 df[df['Label'] == 'bad']['url_segments']]\n",
    "ax4.hist([segments_data[0], segments_data[1]], bins=range(0, min(20, max(df['url_segments'])+2)), \n",
    "         alpha=0.7, label=['Good URLs', 'Bad URLs'], color=colors)\n",
    "ax4.set_xlabel('URL Segments Count', fontweight='bold')\n",
    "ax4.set_ylabel('Frequency', fontweight='bold')\n",
    "ax4.set_title('URL Segments Distribution', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Feature Correlation Heatmap\n",
    "ax5 = plt.subplot(2, 3, 6)\n",
    "correlation_matrix = df[m4_features].corr()\n",
    "im = ax5.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "ax5.set_xticks(range(len(m4_features)))\n",
    "ax5.set_yticks(range(len(m4_features)))\n",
    "ax5.set_xticklabels([f.replace('_', '\\n') for f in m4_features], rotation=45, ha='right')\n",
    "ax5.set_yticklabels([f.replace('_', '\\n') for f in m4_features])\n",
    "ax5.set_title('Feature Correlations', fontweight='bold')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(m4_features)):\n",
    "    for j in range(len(m4_features)):\n",
    "        text = ax5.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.colorbar(im, ax=ax5, shrink=0.8)\n",
    "\n",
    "plt.suptitle('M4: Subdomain & Path Depth Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(f\"\\nğŸ” Key Insights from M4 Analysis:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"1. Subdomain Analysis:\")\n",
    "print(f\"   - Good URLs average: {good_sub_mean:.2f} subdomains\")\n",
    "print(f\"   - Bad URLs average: {bad_sub_mean:.2f} subdomains\")\n",
    "print(f\"   - Difference: {bad_sub_mean-good_sub_mean:+.2f} subdomains\")\n",
    "\n",
    "good_path_mean = good_urls['path_depth'].mean()\n",
    "bad_path_mean = bad_urls['path_depth'].mean()\n",
    "print(f\"\\n2. Path Depth Analysis:\")\n",
    "print(f\"   - Good URLs average: {good_path_mean:.2f} levels\")\n",
    "print(f\"   - Bad URLs average: {bad_path_mean:.2f} levels\")\n",
    "print(f\"   - Difference: {bad_path_mean-good_path_mean:+.2f} levels\")\n",
    "\n",
    "if bad_sub_mean > good_sub_mean:\n",
    "    print(f\"\\nâœ… Bad URLs use more subdomains - confirms complex structure pattern!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Good URLs use more subdomains - unexpected pattern!\")\n",
    "\n",
    "if bad_path_mean > good_path_mean:\n",
    "    print(f\"âœ… Bad URLs have deeper paths - confirms complex structure pattern!\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸  Good URLs have deeper paths - legitimate sites may have complex structures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c804a",
   "metadata": {},
   "source": [
    "## Feature Summary & Export\n",
    "\n",
    "Create a summary of extracted features and save the data for M5 (Normalization & Scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create M4 feature summary\n",
    "m4_feature_summary = pd.DataFrame({\n",
    "    'Feature': m4_features,\n",
    "    'Description': [\n",
    "        'Number of subdomains in the URL hostname',\n",
    "        'Depth of directory structure in URL path',\n",
    "        'Number of query parameters in URL',\n",
    "        'Binary flag for presence of URL fragment (#)',\n",
    "        'Total number of URL segments separated by /'\n",
    "    ],\n",
    "    'Type': ['Integer', 'Integer', 'Integer', 'Binary', 'Integer'],\n",
    "    'Mean_Good': [good_urls[f].mean() for f in m4_features],\n",
    "    'Mean_Bad': [bad_urls[f].mean() for f in m4_features],\n",
    "    'Min_Value': [df[f].min() for f in m4_features],\n",
    "    'Max_Value': [df[f].max() for f in m4_features]\n",
    "})\n",
    "\n",
    "print(\"M4 Feature Engineering Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(m4_feature_summary.round(3).to_string(index=False))\n",
    "\n",
    "# Display final dataset structure\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
    "print(f\"New M4 features added: {m4_features}\")\n",
    "print(f\"\\nAll dataset columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Feature importance by variance\n",
    "print(f\"\\nğŸ“Š Feature Variance Analysis:\")\n",
    "feature_variance = df[m4_features].var().sort_values(ascending=False)\n",
    "print(feature_variance.round(4))\n",
    "\n",
    "# Save M4 results for M5 (Normalization & Scaling)\n",
    "output_dir = '../results/outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, 'm4_structure_features.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nğŸ“Š Dataset with M1+M2+M3+M4 features saved to: {output_path}\")\n",
    "print(f\"âœ… Ready for M5 (Normalization & Scaling)\")\n",
    "\n",
    "# Save M4 feature summary\n",
    "summary_path = os.path.join(output_dir, 'm4_feature_summary.csv')\n",
    "m4_feature_summary.to_csv(summary_path, index=False)\n",
    "print(f\"ğŸ“‹ M4 Feature summary saved to: {summary_path}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ M4 Analysis Complete: URL structure features (subdomains & path depth) successfully extracted\")\n",
    "\n",
    "# Quick preview of data ready for scaling\n",
    "print(f\"\\nData preview for M5:\")\n",
    "numeric_cols = [col for col in df.columns if col not in ['URL', 'Label']]\n",
    "print(f\"Numeric features ready for scaling: {len(numeric_cols)} features\")\n",
    "print(f\"Features: {numeric_cols}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
